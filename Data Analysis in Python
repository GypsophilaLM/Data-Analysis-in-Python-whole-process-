#importing a CSV into Python
import pandas as pd
url = "http://..."
df = pd.read_csv(url, header = None)

#printing the dataframe in Python
#print the entire dataframe
df

#show the first n rows of datadrame
df.head(n)

#show the bottom n rows of datadrame
df.tail(n)

#replace default header
df.columns = headers

#exporting to different formats in Python
Data Format    Read               Save
CSV            pd.read_csv()      df.to_csv()
JSON           pd.read_json()     df.to_json()
EXCEL          pd.read_excel()    df.to_excel()
SQL            pd.read_sql()      df.to_sql()

#check data types in pandas
df.dtypes

#check statistical summary
df.describe()

Dealing with Missing Values in Python
#drop missing values in Python(axis=0 drops the entire row; axis=1 drops the entire column)
df.dropna(subset=["price"], axis=0, inplace=True)

#how to replace missing values in Python
i.e. mean = df["normalized-losses"].mean
     df["normalized-losses"].replace(np.nan, mean)
     #where 'np.nan' is the missing_value and 'mean' is the new_value

Data Formatting in Python
#convert "mpg" to "L/100km" in car dataset
df["city_mpg"] = 235/df["city_mpg"]
#rename the column
df.rename(columns = {"city_mpg": "city-L/100km"}, inplace=True)

Mothods of Normalizing Data
1. simple feauture scaling
2. min-max
3. z-score

Binning in Python:
#we would like 3 bins of equal bin width, so we need 4 numbers as dividers that are equal distance apart
bins = np.linspace(min(df["price"]), max(df["price"]),4)
group_names = ["Low","Medium","High"]
df["priced-binned"] = pd.cut(df["price"], bins, labels=group_names, include_lowest=True)

Dummy Variables in Python pandas
#transform categorical variable to dummy variables(0 or 1)
pd.get_dummies(df['fuel'])

#descriptive statistics
df.describe()

#summarize the categorical data
drive_wheels_counts = df["drive-wheels"].value_counts()
drive_wheels_counts.rename(columns = {'drive-wheels':'value_counts', inplace=True})
drive_wheels_counts.index.name = 'drive-wheels'

#box-plot
sns.boxplot(x="drive-wheels", y="price", data=df)

#scatter-plot
y = df["price"]
x = df["engine-size"]
plt.scatter(x,y)

plt.title("Scatterplot of Engine Size vs Price")
plt.xlabel("Engine-Size")
plt.ylabel("Price")

#GroupBy in Python
dt_test = df[['drive-wheels', 'body-style', 'price']]
df_grp = df_test.groupby(['drive-wheels', 'body-style'], as_index=False).mean()
df_grp

#pivot table
df_pivot = df_grp.pivot(index='drive-wheels', columns='body-style')

#heatmap
plt.pcolor(df_pivot, cmap='RdBu')
plt.colorbar()
plt.show()

correlation
#correlation between 2 features
sns.regplot(x="engine-size", y="price", data=df)
plt.ylim(0,)

Statistical Correlation Methods:
1. Pearson correlation
   it gives u 2 values: correlation coefficient & p-value
   #e.g. pearson_coef, p-value = stats.pearsonr(df['horsepower'], df['price'])

Model Development:
1. linear regression
   steps in python:
   #import linear_model from scikit-learn
   from scikit.linear_model import LinearRegression
   
   #create a Linear Regression Object using the instructor
   lm = LinearRegression()
   
   #define the predictor variable and target variable
   x = df[['highway-mpg']]
   y = df['price']
   
   #then use lm.fit(X.Y) to fit the model, ie.e find the parameters b0 & b1
   lm.fit(X.Y)
   
   #obtain a prediction
   yhat = lm.predict(x)
2. multiple linear regression
   #extract the for 4 predictor variables and store them in the variable Z
   z = df[['horsepower', 'curb-weight', 'engine-size', 'highway']]
   
   #train the model as before
   lm.fit(z, df['price'])
   
   #obtain a prediction
   yhat = lm.predict(x)
 
regression plot
import seaborn as sns
sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0,)

residual plot
subtracting the predicted value from the actual value

3. polynomial regression
   this method is beneficial for describing curvilinear relationships
   quadratic - 2nd order
   cubic - 3rd order
   higher order
   
   #calculate polynomial of 3rd order
   f = np.polufit(x,y,3)
   p = np.poly1d(f)
   
   #print out the model
   print(p)
   
   polynomial regression with more than 1 dimension:
   #the "preprocessing" library in scikit-learn
   from sklearn.preprocessing import PolynomialFeatures
   pr = PolynomialFeatures(degree=2, include_bias=False)
   x_polly = pr.fit_transform(x[['horsepower','curb-weight']])
   
   #normalize the each feature simultaneously
   from sklearn.preprocessing import StandardScaler
   SCALE = StandardScaler()
   SCALE.fit(x_data[['horsepower','curb-weight']])
   x_scale = SCALE.transform(x_data[['horsepower','curb-weight']])

Measures for in-sample evaluation
2 important measures to determine the fit of a model:
1. Mean Squared Error(MSE)
   #in python
   from sklearn.metric import mean_squared_error
   mean_squared_error(actual_value, predicted_value)
2. R-squared(R^2)
   #in python

Model Evaluation and Refinement
#split data into random train and test subsets
#x_data: features or independent variables
#y_data: dataset target: df['price']
#x_train, y_train: parts of available data as training set
#x_test, y_test: parts of available data as testing set
#test_size: percentage of the data for testing
#random_state: number generator used for rando sampling
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=0)

cross validation
#lr represents the linear regression model
#cv=3 means that the dataset is split into 3 equal partitions
from sklearn.model_selection import cross_val_score
scores = cross_val_score(lr, x_data, y_data, cv=3)
np.mean(scores)

Ridge regression
controls the magnitude of these polynomial coefficients by introducing the parameter Alpha
from sklearn.linear_model import Ridge
#here, we have a group of predicted data in order to select Alpha
RidgeModel = Ridge(alpha=0.1)
RidgeModel.fit(X,y)
Yhat = RidgeModel.predict(x)

Hyperparameters
Alpha in Ridge regression is called a hyperparameter
Scikit-lean has a means of automatically iterating over these hyperparameters using cross-validation valled Grid Search
#we select the hyperparameter that minimize the errors
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
parameters1 = [{'alpha': [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]
RR = Ridge()
Grid1 = GridSearchCV(RR,parameters1, cv=4) #where folds=4
Grid1.fit(x_data[['horsepower','cube-weight','engine-size','highway-mpg']],y_data)
Grid1.best_estimator_
scores = Grid1.cv_results_
scores['mean_test_score']

